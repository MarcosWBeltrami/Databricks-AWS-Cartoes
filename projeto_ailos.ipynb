{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6636a328-a305-4df4-b925-3bf7088fbe43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: boto3 in /databricks/python3/lib/python3.9/site-packages (1.21.32)\nRequirement already satisfied: botocore<1.25.0,>=1.24.32 in /databricks/python3/lib/python3.9/site-packages (from boto3) (1.24.32)\nRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /databricks/python3/lib/python3.9/site-packages (from boto3) (0.5.0)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.9/site-packages (from boto3) (0.10.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.32->boto3) (2.8.2)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /databricks/python3/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.32->boto3) (1.26.9)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.32->boto3) (1.16.0)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28397f31-54b5-43f5-b216-fbe163124d7b",
     "showTitle": true,
     "title": "Imports de funções"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from botocore.exceptions import ClientError\n",
    "import datetime\n",
    "import re\n",
    "import boto3\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520b77fd-f67d-44f7-afb9-eaf48b9831bd",
     "showTitle": true,
     "title": "Configuração dos acessos"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://projeto-mw/CEXT_7562011_20240125_0002504.CCB\ns3a://projeto-mw/camada/\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"projeto-mw\"\n",
    "files = dbutils.fs.ls(f\"s3a://{bucket_name}/\")\n",
    "for f in files:\n",
    "    print(f.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae67ff8-b145-4bf4-aaf8-7d25b68ac268",
     "showTitle": true,
     "title": "Acesso ao AWS S3"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://projeto-mw/CEXT_7562011_20240125_0002504.CCB\ns3://projeto-mw/camada/\n"
     ]
    }
   ],
   "source": [
    "# Definir a região do AWS S3\n",
    "aws_region = \"us-east-2\" \n",
    "spark.conf.set(\"fs.s3a.endpoint\", f\"s3.{aws_region}.amazonaws.com\")\n",
    "\n",
    "# Listar arquivos no bucket S3 fonte\n",
    "caminho = \"s3://projeto-mw/CEXT_7562011_20240125_0002504.CCB\"\n",
    "bucket_source = \"s3://projeto-mw\"\n",
    "arquivos = dbutils.fs.ls(bucket_source)\n",
    "\n",
    "# Exibir os arquivos encontrados\n",
    "for arquivo in arquivos:\n",
    "    print(arquivo.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c295ad-2fc9-4246-a7b7-38b8a6a72593",
     "showTitle": true,
     "title": "Acesso ao Segredo"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segredo obtido: {'username': 'ailos', 'password': 'admin', 'engine': 'oracle', 'host': 'ailos.cdy2k46c4jd1.us-east-2.rds.amazonaws.com', 'port': 1521, 'dbname': 'ORCL', 'dbInstanceIdentifier': 'ailos'}\n"
     ]
    }
   ],
   "source": [
    "secrets_client = boto3.client(\n",
    "    \"secretsmanager\",\n",
    "    region_name=\"us-east-2\",  \n",
    "    aws_access_key_id=\"AKIA3FLD4CWPAVORC5FN\",  \n",
    "    aws_secret_access_key=\"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\"\n",
    ")\n",
    "\n",
    "nome_segredo = \"ailos_segredo\" \n",
    "response = secrets_client.get_secret_value(SecretId=nome_segredo)\n",
    "\n",
    "import json\n",
    "segredo = json.loads(response[\"SecretString\"])\n",
    "\n",
    "print(\"Segredo obtido:\", segredo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "279547b0-bf2c-4c8e-81fb-aacb2f5b6390",
     "showTitle": true,
     "title": "Acesso ao RDS"
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:oracle:thin:@ailos.cdy2k46c4jd1.us-east-2.rds.amazonaws.com:1521/ORCL\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"ailos\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"oracle.jdbc.OracleDriver\"\n",
    "}\n",
    "\n",
    "df = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", \"jdbc:oracle:thin:@ailos.cdy2k46c4jd1.us-east-2.rds.amazonaws.com:1521/ORCL\")\n",
    "     .option(\"dbtable\", \"schema.ailos\")\n",
    "     .option(\"user\", \"ailos\")\n",
    "     .option(\"password\", \"admin\")\n",
    "     .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe4617f-ebfd-4379-b422-0461aa7f0d29",
     "showTitle": true,
     "title": "Verificação do nome do Arquivo"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado salvo como JSON no S3: s3a://projeto-mw/camada/bronze/resultado_validacao.json\n"
     ]
    }
   ],
   "source": [
    "nome_extracao = caminho.split(\"/\")[-1]\n",
    "\n",
    "resultado = \"ok\" if nome_extracao.startswith(\"CEXT_756\") and nome_extracao.endswith(\".CCB\") else \"nok\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"nome_arquivo\", StringType(), True),\n",
    "    StructField(\"resultado\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (nome_extracao, resultado)\n",
    "], schema)\n",
    "\n",
    "\n",
    "caminho_s3 = \"s3a://projeto-mw/camada/bronze/resultado_validacao.json\"\n",
    "df.write.json(caminho_s3, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Resultado salvo como JSON no S3: {caminho_s3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3721acf1-6636-458b-a173-947eddf16fb3",
     "showTitle": true,
     "title": "Verificação do conteúdo do Arquivo"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log salvo como JSON no S3: s3a://projeto-mw/camada/bronze/logs/resultado_validacao.json\nLog salvo como JSON no S3: s3a://projeto-mw/camada/silver/logs/\nLog salvo como JSON no S3: s3a://projeto-mw/camada/gold/logs/\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(bucket_source)\n",
    "linhas = df.collect()\n",
    "\n",
    "if len(linhas) < 2:\n",
    "    resultado = \"Erro\"\n",
    "    mensagem = \"Arquivo deve ter pelo menos duas linhas (cabeçalho e trailer)\"\n",
    "else:\n",
    "    cabecalho = linhas[0][0]\n",
    "    trailer = linhas[-1][0]\n",
    "\n",
    "    if not cabecalho.startswith(\"CEXT0\"):\n",
    "        resultado = \"Erro\"\n",
    "        mensagem = \"Cabeçalho inválido\"\n",
    "    elif not trailer.startswith(\"CEXT9\"):\n",
    "        resultado = \"Erro\"\n",
    "        mensagem = \"Trailer inválido\"\n",
    "    else:\n",
    "        resultado = \"Sucesso\"\n",
    "        mensagem = \"Estrutura do arquivo é válida\"\n",
    "\n",
    "# Capturar a data e hora do início\n",
    "data_inicio = datetime.datetime.now()\n",
    "\n",
    "# Criar um DataFrame para o log\n",
    "log_schema = StructType([\n",
    "    StructField(\"data_inicio\", StringType(), True),\n",
    "    StructField(\"data_fim\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"mensagem\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Criar o DataFrame de log\n",
    "log_df = spark.createDataFrame([\n",
    "    (str(data_inicio), None, resultado, mensagem)\n",
    "], log_schema)\n",
    "\n",
    "# Atualizar a data de fim e salvar no S3 ou na camada bronze\n",
    "data_fim = datetime.datetime.now()\n",
    "log_df = log_df.withColumn(\"data_fim\", F.lit(str(data_fim)))\n",
    "\n",
    "\n",
    "caminho_log = \"s3a://projeto-mw/camada/bronze/logs/resultado_validacao.json\"\n",
    "caminho_silver = \"s3a://projeto-mw/camada/silver/logs/\"\n",
    "caminho_gold = \"s3a://projeto-mw/camada/gold/logs/\"\n",
    "log_df.write.json(caminho_log, mode=\"overwrite\")\n",
    "log_df.write.json(caminho_silver, mode=\"overwrite\")\n",
    "log_df.write.json(caminho_gold, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Log salvo como JSON no S3: {caminho_log}\")\n",
    "print(f\"Log salvo como JSON no S3: {caminho_silver}\")\n",
    "print(f\"Log salvo como JSON no S3: {caminho_gold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae71795-03f1-401f-aff7-c7da8ff88caa",
     "showTitle": true,
     "title": "Validação se os arquivos foram criados corretamente"
    }
   },
   "outputs": [],
   "source": [
    "caminho = \"s3://projeto-mw/CEXT_7562011_20240125_0002504.CCB\"\n",
    "bucket_source = \"s3://projeto-mw\"\n",
    "arquivos = dbutils.fs.ls(bucket_source)\n",
    "\n",
    "# Exibir os arquivos encontrados\n",
    "for arquivo in arquivos:\n",
    "    print(arquivo.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd33474b-6672-4c4c-bf91-5eabaf90a589",
     "showTitle": true,
     "title": "Oracle"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:oracle:thin:@ailos.cdy2k46c4jd1.us-east-2.rds.amazonaws.com:1521/ORCL\"\n",
    "properties = {\n",
    "    \"user\": \"ailos\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"oracle.jdbc.OracleDriver\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"DUAL\").load()\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    print(\"Erro ao conectar\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66a9ede-97b5-451d-b96e-2c9ce9468ff5",
     "showTitle": true,
     "title": "Oracle"
    }
   },
   "outputs": [],
   "source": [
    "# Query para buscar ID do controle de arquivos\n",
    "query_id_controle = \"\"\"\n",
    "SELECT IDARQUIVO_CONTROLE\n",
    "FROM CARTOES.TB_ARQUIVO_CONTROLE\n",
    "WHERE NRARQUIVO = 2\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_id_controle = (spark.read.format(\"jdbc\")\n",
    "            .option(\"url\", jdbc_url)\n",
    "            .option(\"query\", query_id_controle)\n",
    "            .option(\"user\", jdbc_properties[\"user\"])\n",
    "            .option(\"password\", jdbc_properties[\"password\"])\n",
    "            .option(\"driver\", jdbc_properties[\"driver\"])\n",
    "            .load())\n",
    "    df_id_controle = (spark.read.format(\"jdbc\")\n",
    "                  .option(\"url\", jdbc_url)\n",
    "                  .option(\"query\", query_id_controle)\n",
    "                  .option(\"user\", jdbc_properties[\"user\"])\n",
    "                  .option(\"password\", jdbc_properties[\"password\"])\n",
    "                  .option(\"driver\", jdbc_properties[\"driver\"])\n",
    "                  .load())\n",
    "    # Verificar se obtivemos um resultado\n",
    "    if df_id_controle.count() == 0:\n",
    "        raise Exception(\"Nenhum IDARQUIVO_CONTROLE encontrado para NRARQUIVO 2\")\n",
    "\n",
    "    # Obter o IDARQUIVO_CONTROLE\n",
    "    id_controle = df_id_controle.first()[\"IDARQUIVO_CONTROLE\"]\n",
    "except Exception as e:\n",
    "    print(\"Erro ao conectar\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7badd1-57ab-4d44-bcf5-f6541dff2acd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Número do cartão para verificar\n",
    "nr_cartao = '5151070044387015'  # Número do cartão que deseja verificar\n",
    "\n",
    "# Query para buscar o número do cartão\n",
    "query = f\"\"\"\n",
    "SELECT 1\n",
    "FROM CARTOES.TB_CARTAO\n",
    "WHERE NRCARTAO = {nr_cartao}\n",
    "\"\"\"\n",
    "\n",
    "# Executar a query no banco de dados\n",
    "result = (spark.read.format(\"jdbc\")\n",
    "          .option(\"url\", jdbc_url)\n",
    "          .option(\"query\", query)\n",
    "          .option(\"user\", jdbc_properties[\"user\"])\n",
    "          .option(\"password\", jdbc_properties[\"password\"])\n",
    "          .option(\"driver\", jdbc_properties[\"driver\"])\n",
    "          .load()\n",
    ")\n",
    "\n",
    "# Verificar se o resultado contém pelo menos uma linha\n",
    "if result.count() > 0:\n",
    "    print(\"Número de cartão encontrado.\")\n",
    "else:\n",
    "    print(\"Número de cartão não encontrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47024f31-1e4e-441b-923d-b0cb1f82fdb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query para buscar ID do controle de arquivos\n",
    "query_id_controle = \"\"\"\n",
    "SELECT IDARQUIVO_CONTROLE\n",
    "FROM CARTOES.TB_ARQUIVO_CONTROLE\n",
    "WHERE NRARQUIVO = 2\n",
    "\"\"\"\n",
    "\n",
    "# Executar a query no banco de dados\n",
    "df_id_controle = (spark.read.format(\"jdbc\")\n",
    "                  .option(\"url\", jdbc_url)\n",
    "                  .option(\"query\", query_id_controle)\n",
    "                  .option(\"user\", jdbc_properties[\"user\"])\n",
    "                  .option(\"password\", jdbc_properties[\"password\"])\n",
    "                  .option(\"driver\", jdbc_properties[\"driver\"])\n",
    "                  .load())\n",
    "\n",
    "# Verificar se obtivemos um resultado\n",
    "if df_id_controle.count() == 0:\n",
    "    raise Exception(\"Nenhum IDARQUIVO_CONTROLE encontrado para NRARQUIVO 2\")\n",
    "\n",
    "# Obter o IDARQUIVO_CONTROLE\n",
    "id_controle = df_id_controle.first()[\"IDARQUIVO_CONTROLE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4fb1c30-75c5-42cb-b81f-0b7d0d06c88a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# Criar um DataFrame para inserir na tabela TB_ARQUIVO_LINHA\n",
    "df_insert_linhas = spark.createDataFrame(\n",
    "    [Row(\n",
    "        IDARQUIVO_LINHA=uuid.uuid4().bytes,\n",
    "        IDARQUIVO=id_arquivo,\n",
    "        NRLINHA=linha[\"NRLINHA\"],\n",
    "        DSCONTEUDO=linha[\"DSCONTEUDO\"],\n",
    "        DTPROCESSO=datetime.datetime.now(),\n",
    "        CDSITUACAO=1\n",
    "    ) for linha in linhas]\n",
    ")\n",
    "\n",
    "# Inserir as linhas no banco de dados\n",
    "df_insert_linhas.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"CARTOES.TB_ARQUIVO_LINHA\") \\\n",
    "    .option(\"user\", jdbc_properties[\"user\"]) \\\n",
    "    .option(\"password\", jdbc_properties[\"password\"]) \\\n",
    "    .option(\"driver\", jdbc_properties[\"driver\"]) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56467c86-1bdd-4c46-8aea-cef6110b80cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "projeto_ailos",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
